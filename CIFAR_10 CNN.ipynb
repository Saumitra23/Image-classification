{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "#one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#use image data generator for image augmentations\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for reproducibility \n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "print('training data shape:',train_images.shape, train_labels.shape)\n",
    "print('testing data shape:',test_images.shape, test_labels.shape)\n",
    "\n",
    "#find the unqiue number of train labels\n",
    "classes = np.unique(train_labels)\n",
    "classes_num = len(classes)\n",
    "print('total number of outputs:',classes_num)\n",
    "print('output classes:',classes)\n",
    "\n",
    "plt.figure(figsize = [4,2])\n",
    "\n",
    "#display the first image in the training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_images[0,:,:],cmap='gray')\n",
    "plt.title('first train image : {}'.format(train_labels[0]))\n",
    "\n",
    "#display the first image in the test data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_images[0,:,:],cmap='gray')\n",
    "plt.title('first test image : {}'.format(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the shape of images and create the variable input_shape\n",
    "Rows, Cols, Dims = train_images.shape[1:]\n",
    "train_data = train_images.reshape(train_images.shape[0], Rows, Cols, Dims)\n",
    "test_data = test_images.reshape(test_images.shape[0], Rows, Cols, Dims)\n",
    "input_shape = (Rows, Cols, Dims)\n",
    "\n",
    "#Change to float datatype\n",
    "\n",
    "train_data = train_data.astype('float32')\n",
    "test_data = test_data.astype('float32')\n",
    "\n",
    "#scale the data to lie b/w 0 to 1\n",
    "train_data /= 255\n",
    "test_data /= 255\n",
    "\n",
    "#change the labels from integer to categorical data\n",
    "train_labels_one_hot = to_categorical(train_labels)\n",
    "test_labels_one_hot = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the new labels\n",
    "print('original label 0',train_labels[0])\n",
    "print('after one hot encoding',train_labels_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv layer, max pooling layer, dropout layer, Dense layer\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    # The first two layers with 32 filters of window size 3*3\n",
    "    model.add(Conv2D(32, (3,3), padding='same',activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    #other layers use 64 filters\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding='same',activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(classes_num, activation='softmax'))#for classification\n",
    "    #among 10 classes\n",
    "    \n",
    "    return model\n",
    "#We have used 6 conv layers with 1 fully-connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "model1 = createModel()\n",
    "\n",
    "#set training process parameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "#set the training configurations: optimizer, Loss function, accuracy metrics\n",
    "model1.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics\n",
    "              =['accuracy'])\n",
    "\n",
    "#view model architecture\n",
    "model1.summary()\n",
    "\n",
    "history = model1.fit(train_data,train_labels_one_hot,\n",
    "                    batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                    validation_data = (test_data, test_labels_one_hot))\n",
    "\n",
    "#check the model results on the test set\n",
    "model1.evaluate(test_data, test_labels_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting the test and training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss vs test loss\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss','Validation loss'],fontsize=18)\n",
    "plt.xlabel('epochs',fontsize=15)\n",
    "plt.ylabel('loss',fontsize=15)\n",
    "plt.title('loss curve',fontsize=17)\n",
    "\n",
    "#training acccuracy vs the test accracy\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training accuracy','Validation accuracy'],fontsize=18)\n",
    "plt.xlabel('epochs',fontsize=15)\n",
    "plt.ylabel('accuracy',fontsize=15)\n",
    "plt.title('accuracy curve',fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we can see there is still overfitting in both the\n",
    "curves even after using the dropout layer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>\n",
    "    One of the major reasons for overfitting is that we donâ€™t have enough data to train our network. Apart from regularization, another very effective way to counter Overfitting is Data Augmentation. It is the process of artificially creating more images from the images we already have by changing the size, orientation etc of the image. It can be a tedious task but fortunately, this can be done in Keras using the ImageDataGenerator instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model2 = createModel()\n",
    "\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics\n",
    "              =['accuracy'])\n",
    "\n",
    "# set training process parameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "# define transformations for train data\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             # randomly shifts images horizontally(fraction of total width)\n",
    "                             height_shift_range=0.1,\n",
    "                             # randomly shifts images vertically\n",
    "                             horizontal_flip=True,vertical_flip=False)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "history2 = model2.fit(datagen.flow(train_data, train_labels_one_hot,\n",
    "                                  batch_size=batch_size),\n",
    "                                  steps_per_epoch=int(np.ceil(train_data.shape[0] / float(batch_size))),\n",
    "                                  epochs=epochs,validation_data=(test_data,test_labels_one_hot),\n",
    "                                  workers=4)\n",
    "\n",
    "model2.evaluate(test_data,test_labels_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting results after data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history2.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history2.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['training loss','testing loss'],fontsize=15)\n",
    "plt.xlabel('epochs',fontsize=15)\n",
    "plt.ylabel('loss',fontsize=15)\n",
    "plt.title('loss curve',fontsize=18)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history2.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history2.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['training accuracy','testing accuracy'],fontsize=15)\n",
    "plt.xlabel('epochs',fontsize=15)\n",
    "plt.ylabel('accuracy',fontsize=15)\n",
    "plt.title('accuracy curve',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above figures clearly show that there is no overfitting, even the test accuracy is greater than that of train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
